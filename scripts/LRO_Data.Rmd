```{r setup, include=F, echo = F}
library(tidyverse)
library(ggplot2)
library(lubridate)
library(RColorBrewer)
library(here)
library(reticulate)

py_require("pip")
py_require("hydroserverpy")
py_require("datetime")


knitr::opts_knit$set(root.dir = here("outputs", "htmls"))
```

## Overview

This script performs initial data cleanining, visualization, and basic analysis for five test sites in the Logan River Observatory network. These sites include: 

- Logan River at Franklin Basin (discharge, water temp)
- Logan River at Main Street (discharge, water temp)
- Logan River at Mendon (discharge, water temp)
- Franklin Basin Climate Station (Precipitation, Air temp)
- Logan River Golf Course Climate Station (Precipitation, Air temp)

Data is called directly from the LRO Hydroserver API (https://lro.hydroserver.org/browse)

This script performs the following actions: 

1. Read in and clean data files
  + Call Metadata for all sites in the LRO network
  + Filter out desired sites
  + Import data using a for loop
2. Plotting and basic analysis
  + Discharge
  + Stream Temperature

## I. Connect to Hydroserver and import data

First, Import necessary packages; establish connection to Hydroserver

```{python}
import hydroserverpy
import datetime
import pandas as pd

from datetime import datetime

from hydroserverpy import HydroServer

#initialize Hydroserver Connection with login Credentials **NOTE** you may need to update these to your Hydroserver login info

hs_api = HydroServer(host = 'https://lro.hydroserver.org', email = 'ggianniny@gmail.com',password='syxqyz-Zeqpor-7naxki')

#Load info for all datastreams:

datastreams = hs_api.datastreams.list(fetch_all = True)

```

### A. Call metadata for all "datastreams" (i.e. all parameters at all sites) in the LRO network

Setup empty data frame to recieve data:

```{python}
df = pd.DataFrame(data = None, index = None, columns = ['uid', 'name', 'property','propCode','unit', 'medium', 'procLevel']) #initialize empty dataframe (outside loop)
```

Read in data with a for-loop (NOTE - this will take a few minutes to run)

```{python}
for i in range(0, len(datastreams.items)):
        uid = str(datastreams.items[i].uid) #extract UID
        name = str(datastreams.items[i].thing.name) #extract site name
        prop = str(datastreams.items[i].observed_property.name) #extract measured property
        propCode = str(datastreams.items[i].observed_property.code) #extract measured property code
        unit = str(datastreams.items[i].unit.name) #extract unit
        medium = str(datastreams.items[i].sampled_medium) #extract sampled medium
        procLevel = str(datastreams.items[i].processing_level.definition) #extract processing level
        siteInfo = pd.DataFrame(data = {'uid':[uid], 'name':[name], 'property':[prop], 'propCode':[propCode], 'unit':[unit],'medium':[medium], 'procLevel':[procLevel]}) #convert uid, name, and unit into dataframe 
        df = pd.concat([df, siteInfo]) #add siteInfo to df
        print("working on"+" "+str(i)) #print current site for progress tracking
```

### B. Filter out desired sites/parameters

First - convert to python, filter out quality controlled data, create reference DF's

```{r}
r_df <- py_to_r(py$df)%>% #converting to R for easier wrangling
  filter(procLevel == "Quality controlled data"|procLevel == "Derived products")%>% #extract quality controlled and derived product data only
  arrange(name, property)%>% #sort by name and unit for easier viewing
  mutate(name = as.factor(name), 
         property = if_else(property == "Temperature", paste(medium, property, sep = " "), property), 
         property = if_else(str_detect(propCode, "WaterYr"), paste(property, "WaterYr", sep = " "), property),
         property = if_else(str_detect(propCode, "Discharge"), propCode, property),
         property = as.factor(property))%>%
  mutate(siteNum = as.numeric(name), 
         propNum = as.numeric(property))

nameRef <- r_df%>%
  select(name, siteNum)%>%
  distinct()%>% #Create dataframe with name references
  arrange(name)

propRef <- r_df%>%
  select(property, propNum)%>%
  distinct()%>%
  arrange(property)#Creste dataframe with property references
```

Next: view reference dataframes, write down site numbers and property numbers for export 

View name references:

```{r}
View(nameRef)
```

Enter desired site numbers below: 

```{r}
soi <- c(5,6,15,16,19) #ENTER DESIRED SITE NUMBERS HERE inside the parentheses. e.g. if you want sites 1, 3, and 19, the code should read "soi <- c(1, 3, 19)"
```

View property references:

```{r}
View(propRef)
```

Enter desired property numbers below:

```{r}
poi <- c(1,3,11,23) #ENTER DESIRED PROPERTY NUMBERS HERE inside the parentheses. e.g. if you want properties 8, 17, and 21, the code should read "soi <- c(8, 17, 21)"
```

Extract desired metadata:

```{r}
#extract desired datastreams:
toDownload <- r_df %>%
  filter(
    siteNum %in% soi&propNum %in% poi
  )

#Check to ensure all sites/data are present: 
View(toDownload)

#convert back to Python for download: 

pyDownload <- r_to_py(toDownload)
```


### C. Import data using a for loop

Setup:

```{python}
#Read "to download" dataframe into Python environment
pyDownload = pd.DatFrame(r.pyDownload)

#initialize empty dataframe to receive data: 

fullData = pd.DataFrame(data = None, index = None, columns = ['phenomenon_time', 'result', 'site','param', 'unit']) #initialize empty dataframe (outside loop)
```

Read in all data using a For loop - NOTE, this will take a while to run (~2-4 min per site)

```{python}
for var in range(0, len(pyDownload["uid"])): #iterate on "i" over # rows in the pyDownload DF
  print("started"+" "+pyDownload.iloc[var,1]) #print current site name for progress tracking
  data = hs_api.datastreams.get_observations(fetch_all = True, uid = pyDownload.iloc[var,0]).dataframe #read in data to temporary df
  data['site'] = pyDownload.iloc[var,1] #add site name
  data['param'] = pyDownload.iloc[var,2] #add parameter name
  data['unit'] = pyDownload.iloc[var,4] #add unit 
  fullData = pd.concat([fullData, data]) #add data to fullData df created in block above. 
  print("finished"+" "+pyDownload.iloc[var,1]) #print current site name for progress tracking
```

Convert full data file back to R for additional wrangling and analysis; replace -9999 values with NA: 

```{r}
fullData <- py_to_r(py$fullData)%>%
  rename(parameterValue = result, dateTime = phenomenon_time, parameter = param)%>% #renaming columns
  mutate(dateTime = ymd_hms(dateTime), #convert date time to POSIXct
         parameterValue = ifelse(parameterValue == -9999, NA, parameterValue)) #replace "no data" value o f-9999 with NA
```

Check date ranges for each site: 

```{r}
dateRanges <- fullData%>%
  group_by(site, parameter)%>%
  summarise(start = min(dateTime), 
            end = max(dateTime))
dateRanges
```

### D. Alternative formatting for DOY plotting:

Alternative data format for DOY plots: 

```{r}
dailyData <- fullData%>%
  mutate(parameter = ifelse(parameter == "Discharge_cfs", "Discharge", parameter),
    siteParam = paste(site, parameter, sep = " "))%>% #add new Site + parameter column for grouping
  mutate(doy = yday(dateTime), #make new DOY column
         date = ymd(substr(dateTime, 1,10)))%>% #make new column with just date
  group_by(siteParam, date, unit, parameter, doy)%>%
  summarise(paramDaily = mean(parameterValue, na.rm = TRUE))%>% #calculate daily average
  group_by(siteParam, doy)%>% #group by day of year
  mutate(paramMin = min(paramDaily, na.rm = T), 
         paramMax = max(paramDaily, na.rm = T), 
         paramMed = median(paramDaily, na.rm = T), 
         paramMean = mean(paramDaily, na.rm = T), 
         qt1 = quantile(paramDaily, probs = 0.2, na.rm = T)[[1]], 
         qt2 = quantile(paramDaily, probs = 0.4, na.rm = T)[[1]], 
         qt3 = quantile(paramDaily, probs = 0.6, na.rm = T)[[1]], 
         qt4 = quantile(paramDaily, probs = 0.8, na.rm = T)[[1]], )%>% #calculate day of year minimum, maximum, median, and mean, plus quantiles for each doy
  ungroup()%>% #ungroup
  arrange(siteParam, date) #sort by site+parameter and date for easier checking
```



## II. Time Series Plots

### A. Discharge

Starting with a basic time series, y = discharge CFS, x = date Time, color = site. 

```{r}
#color palate: 

discharge.pal <- c("#2171B5", "#08306B",  "#9ECAE1")

#Check with updated plot: 
q.comb <- ggplot(
  filter(fullData, parameter == "Discharge_cfs"), #extract discharge data
  aes(x = dateTime, y = parameterValue, color = site) #setting mapping
)+
  geom_line()+
  theme_classic()+
  labs(x = "Date", y = "Discharge, CFS", color = "Station Name")+ #axis labels
  scale_color_manual(values = discharge.pal)+ #Setting color palate
  theme(legend.position = "bottom", 
        axis.title = element_text(size = 10, face = "bold"), 
        legend.title = element_text(size = 10, face = "bold")) #editing text appearance 

q.comb

#Save: 

ggsave("Discharge_Comb.jpg", q.comb, device = "jpeg", path = here("outputs", "plots"), units = "in", dpi = "retina", height = 6, width = 11)
```

### B. Water Temperature

Basic Time Series:

```{r}
#Initial plot:
stream.temp <- ggplot(
  filter(fullData,
         parameter == "Surface Water Temperature"), 
  aes(x = dateTime, y = parameterValue, color = site)
)+
  geom_line()+
  theme_classic()+
  labs(x = "Date", y = "Stream Temperature, deg. C", color = "Station Name")+
  scale_color_manual(values = discharge.pal)+
  theme(legend.position = "bottom", 
        axis.title = element_text(size = 10, face = "bold"), 
        legend.title = element_text(size = 10, face = "bold"))

stream.temp

#Save: 

ggsave("StreamTemp_Comb.jpg", stream.temp, device = "jpeg", path = here("outputs", "plots"), units = "in", dpi = "retina", height = 6, width = 11)
```



### C. Air Temperature

Basic time series:

```{r}
climate.pal <- c("#C7E9C0","#238B45" )

air.temp <- ggplot(
  filter(fullData,
         parameter == "Air Temperature"), 
  aes(x = dateTime, y = parameterValue, color = site)
)+
  geom_line()+
  theme_classic()+
  labs(x = "Date", y = "Air, deg. C", color = "Station Name")+
  scale_color_manual(values = climate.pal)+
  theme(legend.position = "bottom", 
        axis.title = element_text(size = 10, face = "bold"), 
        legend.title = element_text(size = 10, face = "bold"))

air.temp


#Save: 

ggsave("AirTemp_Comb.jpg", air.temp, device = "jpeg", path = here("outputs", "plots"), units = "in", dpi = "retina", height = 6, width = 11)
```

### D. Precipitation

Basic time series:

```{r}
precip <- ggplot(
  filter(fullData,
         parameter == "Precipitation"), 
  aes(x = dateTime, y = parameterValue, color = site)
)+
  geom_line()+
  theme_classic()+
  labs(x = "Date", y = "Cumulative Annual Precipitation, cm", color = "Station Name")+
  scale_color_manual(values = climate.pal)+
  theme(legend.position = "bottom", 
        axis.title = element_text(size = 10, face = "bold"), 
        legend.title = element_text(size = 10, face = "bold"))

precip


#Save: 

ggsave("Precip_Comb.jpg", precip, device = "jpeg", path = here("outputs", "plots"), units = "in", dpi = "retina", height = 6, width = 11)
```


## III. Day of year plotting

Using "dailyData" dataframe created in step I. 

First, Test plotting day of year plot with one site - data prep: 

```{r}
thisyear <- format(Sys.Date(), "%Y") #Create object with this year for later filtering
test <- filter(dailyData, siteParam == "Climate Station at Franklin Basin Air Temperature") #make test dataset with one site/parameter

datemin <- year(min(test$date))
datemax <- year(max(test$date)) #Make date range object for labeling
                   
```

Plotting:

```{r}
testplot <- ggplot()+ 
  geom_line(data = test, aes(x = doy, y = paramMin, color = "Minimum"))+ #draw minimum line 
  geom_line(data = test, aes(x = doy, y = paramMed, color = paste("Median ", "(", datemin, "-", datemax, ")", sep = "")))+ #draw median line, add daterange object for label
  geom_line(data = test, aes(x = doy, y = paramMax, color = "Maximum"))+ #add maximum line
  geom_line(data = filter(test, year(date) == datemax), aes(x = doy, y = paramDaily, color = paste(datemax)))+ #filter out this year's data and add to plot
  scale_color_manual(values = c("black", "blue", "green", "red"))+ #Setting line color scale
  labs(x = "Day of Year", y = "Discharge, CFS", color = "Legend", title = unique(test$siteParam))+ #axis and legend lables
  geom_ribbon(data = test, aes(x = doy, ymax = paramMax, ymin = qt4), fill = "blue", alpha = 0.25)+
  geom_ribbon(data = test, aes(x = doy, ymax = qt4, ymin = qt3), fill = "lightblue", alpha = 0.25)+
  geom_ribbon(data = test, aes(x = doy, ymax = qt3, ymin = qt2), fill = "green", alpha = 0.25)+
  geom_ribbon(data = test, aes(x = doy, ymax = qt2, ymin = qt1), fill = "yellow", alpha = 0.25)+
  geom_ribbon(data = test, aes(x = doy, ymax = qt1, ymin = paramMin), fill = "red", alpha = 0.25)+ #adding shaded ribbons for quantiles
  theme_classic()+ #removing background formatting
  theme(legend.position = c(0.8, 0.75), #repositioning legend within plot area
        axis.text = element_text(size = 10), 
        axis.title = element_text(size = 10, face = "bold"), 
        legend.text = element_text(size = 9), 
        legend.title = element_text(size = 9, face = "bold"), 
        plot.title = element_text(size = 11, face = "bold")) #axis and legend text formatting
testplot
```

Success! Next, iterate over each site/parameter to make all plots: 

Setup: 

```{r}
allSiteParams <- unique(dailyData$siteParam) #make vector of all site/parameter combos
plot.list <- list() #empty list to recieve plots
thisyear <- format(Sys.Date(), "%Y") #Create object with this year for later filtering
q.pal <- c("blue", "lightblue", "green", "yellow", "red")
t.pal <- c("red", "yellow", "green", "lightblue", "blue")
q.line <- c("black", "blue", "green", "red")
t.line <- c("black", "red", "green", "blue") #set up color palate options
dateMins <- year(dateRanges$start)
dateMaxes <- year(dateRanges$end)
```

Make plots with a for-loop:

```{r}
for(i in 1:length(allSiteParams)){
  d <- filter(dailyData, siteParam == allSiteParams[i]) #Filter data to "ith" site/parameter

  fill.pal <- if (str_detect(allSiteParams[[i]], "Temperature")) t.pal else q.pal
  line.pal <- if (str_detect(allSiteParams[[i]], "Temperature")) t.line else q.line
  
  yLab <- paste(
    unique(d$parameter), 
    unique(d$unit), 
    sep = ", "
  )
  
  #Plotting:
  
  p <- ggplot()+ 
  geom_line(data = d, aes(x = doy, y = paramMin, color = "Minimum"))+ #draw minimum line 
  geom_line(data = d, aes(x = doy, y = paramMed, color = paste("Median ", "(", dateMins[[i]], "-", dateMaxes[[i]], ")", sep = "")))+ #draw median line, add daterange object for label
  geom_line(data = d, aes(x = doy, y = paramMax, color = "Maximum"))+ #add maximum line
  geom_line(data = filter(d, year(date) == dateMaxes[[i]]), aes(x = doy, y = paramDaily, color = as.character(dateMaxes[[i]])))+ #filter out this year's data and add to plot
  scale_color_manual(values = line.pal)+ #Setting line color scale
  labs(x = "Day of Year", y = yLab, color = "Legend", title = unique(d$siteParam))+ #axis and legend lables
  geom_ribbon(data = d, aes(x = doy, ymax = paramMax, ymin = qt4, fill = "q1"), alpha = 0.25)+
  geom_ribbon(data = d, aes(x = doy, ymax = qt4, ymin = qt3, fill = "q2"), alpha = 0.25)+
  geom_ribbon(data = d, aes(x = doy, ymax = qt3, ymin = qt2, fill = "q3"), alpha = 0.25)+
  geom_ribbon(data = d, aes(x = doy, ymax = qt2, ymin = qt1, fill = "q4"), alpha = 0.25)+
  geom_ribbon(data = d, aes(x = doy, ymax = qt1, ymin = paramMin, fill = "q5"), alpha = 0.25)+ #adding shaded ribbons for quantiles
    scale_fill_manual(values = fill.pal)+
    guides(fill = "none")+
  theme_classic()+ #removing background formatting
  theme(legend.position = "bottom", #re positioning legend along bottom
        axis.text = element_text(size = 10), 
        axis.title = element_text(size = 10, face = "bold"), 
        legend.text = element_text(size = 9), 
        legend.title = element_text(size = 9, face = "bold"), 
        plot.title = element_text(size = 11, face = "bold")) #axis and legend text formatting
  
  plot.list[[i]]<-p #Save plot as "ith" plot in plot list
  
  fn <- paste(allSiteParams[[i]], ".jpg", sep = "")
  
  ggsave(fn, p, device = "jpeg", path = here("outputs", "plots", "doy_plots"), units = "in", dpi = "retina", height = 6, width = 11)
  
  print(paste("Finished", allSiteParams[[i]], sep = " ")) #Print site name/parameter for tracking
}
```
Check outputs: 

```{r}
plot.list[[1]]
```

### NOT USED: Manually read in and clean data files with r

Make list of all data files in data folder (includes all sites + parameters listed above):

```{r}
#fileNames <- list.files(here("data"), full.names = T)
```

Read in data and store in a list: 

```{r}
#dataList <- list() #empty list to receive data
#mdList <- list() #empty list to receive metadata


#for(i in 1:length(fileNames)){ #loop over fileNames vector
#  mdTemp<-read.csv(fileNames[i], col.names = c("C1")) #read in "ith" file
  
#  mdTemp1<-mdTemp%>% 
#    mutate(rownum = c(1:nrow(mdTemp)))%>% #add rownum column
 #   filter(rownum == 11 | rownum == 57 |rownum == 65)%>% #extract desired metadata rows (station name, parameter name)
  #  mutate(metadata = substr(C1, 9, nchar(C1)))%>% #remove "# Name:" from beginning of metadata entries
   # select(metadata, rownum)%>% #drop extra cols
   # pivot_wider(names_from = rownum, values_from = metadata)%>% #pivot wider to get columns for site name and parameter 
   # rename(site = 1, parameter = 2, units =3)%>% #add column names
   # mutate(dataRef = i) #add reference column for later merge
  
#  mdList[[i]] <- mdTemp1 #store as "ith" item in metadata list
  
 # dataList[[i]]<-read.csv(fileNames[i], skip = 81)%>% #read in "ith" file, skipping 81 lines of metadata
  #  mutate(dataRef = i)%>% #add reference column for merge with metadata
   # mutate(date = substr(ResultTime, 1, 10), #extract date from ResultTime column
    #       time = substr(ResultTime, 12, 19), #extract time from ResultTime column
     #      dateTime = ymd_hms(paste(date, time, sep = " ")))%>% #create new dateTime column in POSIXct format
    #rename(parameterValue = Result)%>% #rename result column
#    select(parameterValue, ResultQualifiers, dateTime, dataRef) #drop extra cols
#}


#head(dataList[[10]]) #check outputs
#head(mdList[[10]])

```

## NOT USED Combine data files for easier plotting

```{r}
#mdAll <-mdList%>%
 # bind_rows()#rbind all metadata files into 1 DF
#dataAll <-dataList%>%
#  bind_rows()#rbind all data files into 1 DF

#fullData <- merge(mdAll, dataAll) #merge metadata and data by dataRef column 

#head(fullData)
#tail(fullData)
```




